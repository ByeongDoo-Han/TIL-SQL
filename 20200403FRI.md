:v:
---
## :heavy_check_mark: 특성공학
- 데이터분석에 필요한 변수의 다양한 시각적 연구
- 변수의 선택 및 제거
- 변수 결합
- 변수 변형

## :heavy_check_mark: 분석에서의 변수의 선택(feature selection)
1. 통계적 접근
전진선택법과 후진선택법은 한번 추가되거나 제거된 변수는 다시 제거 추가가 불가능 
    1) 전진선택법(forward selection) 
        변수를 하나씩 추가하면서 고려, 제로베이스부터 시작
        추가했을 시에 예측력이 증가하면 계속 추가
    2) 후진선택법(backward selection) 
        전체변수를 학습 후 가장 의미없는 변수를 하나씩 제거
    3) STEPWISE 기법
        변수의 선택이 자유롭고 개방적인 모델
        이전 모델보다 예측력이 우수하지 않다면 stop

2. 모델적 접근 : 머신러닝/딥러닝 모델 내부에 변수선택에 대한 내용 포함
ex) DT, RF, NN(Neural Network)
```
Y ~ X1 + X2 + ... + Xn          # 전체변수를 시작으로
Y ~      X2 + ... + Xn          # 하나를 제거
Y ~           X3  + ... + Xn    # 두개째를 제거
Y ~ X1 +    + X3  + ... + Xn    # 첫번 째 제거했던 것을 다시 한번 고려
```
### `step` 함수로 변수선택 수행
```r
install.packages('mlbench')
library(mlbench)
data(BostonHousing) 
m <- lm(medv ~ ., data = BostonHousing)
m2 <- step(m, direction = 'both')
formula(m2)

# AIC 값보다 낮아지는 값이 있다면 
# 그에 해당하는 컬럼은 제거하는 것을 고려해볼 필요가 있다.
m3<- lm(formula(m2), data=BostonHousing)
# 제거한 것으로 다시 적용
```
```
> m2<- step(m, direction='both')

Step:  AIC=1585.76
medv ~ crim + zn + chas + nox + rm + dis + rad + tax + ptratio + 
    b + lstat

          Df Sum of Sq   RSS    AIC
<none>                 11081 1585.8
+ indus    1      2.52 11079 1587.7
+ age      1      0.06 11081 1587.8
- chas     1    227.21 11309 1594.0
- crim     1    245.37 11327 1594.8
- zn       1    257.82 11339 1595.4
- b        1    270.82 11352 1596.0
- tax      1    273.62 11355 1596.1
- rad      1    500.92 11582 1606.1
- nox      1    541.91 11623 1607.9
- ptratio  1   1206.45 12288 1636.0
- dis      1   1448.94 12530 1645.9
- rm       1   1963.66 13045 1666.3
- lstat    1   2723.48 13805 1695.0
```

## :heavy_check_mark: 변수 선택의 모든 경우에 대한 비교

- 적절한 회귀 모델을 선택하는 과정으로 모든 설명변수의 추가와 제거를 고려한 모델의 비교
- 총 N개의 설명 변수가 있을 때 각 변수를 추가하거나 뺀 총 2N개의 회귀 모델을 만들고 이를 모두 비교하는 방법
```r
install.packages('leaps')
library(leaps)
data(BostonHousing)
m <- regsubsets(medv ~ ., data = BostonHousing)
summary(m)

plot(m, scale = 'adjr2')
```
## :heavy_check_mark: 이상치
```r
data(Orange)
m <- lm(circumference ~ age + I(age^2), data = Orange)
rstudent(m)
```
```
> rstudent(m)
          1           2           3           4           5           6 
-0.51060111 -0.42284392 -0.01483104 -0.14994219 -0.73099028 -0.57412471 
          7           8           9          10          11          12 
-1.35250055 -0.41738616 -0.10060219  0.69348765  1.06880880  0.79556985 
         13          14          15          16          17          18 
 1.23881268  0.45913462 -0.51060111 -0.62974506 -0.36729020 -0.35563285 
         19          20          21          22          23          24 
-0.88072728 -0.66330720 -1.51968352 -0.44841898 -0.30537284  0.72345025 
         25          26          27          28          29          30 
 1.41333619  1.00669363  1.42815774  0.80187803 -0.51060111 -0.68926057 
         31          32          33          34          35          36 
-0.19079941  0.14312273 -0.08496138  0.36476152 -0.33864919  5.53843789 
         37          38 
-0.65947737 -1.76073133 
```
### 이상치를 강제로 삽입하고 이상치 검정 수행
```r
Orange <- rbind(Orange, data.frame(Tree=as.factor(c(6,6,6)),
                                   age=c(118,484,664),
                                   circumference=c(177,50,30)))
m <- lm(circumference ~ age + I(age^2), data = Orange)
library('car')
outlier.test(m)
```
36번째 값이 통계적으로 유의한 이상치임을 확인
- 36번째 데이터가 이상치다. Bonferroni p 가 0.05 이하라면 //
```
> outlier.test(m)
 
rstudent unadjusted p-value Bonferonni p
36 5.538438 3.429e-06 0.0001303
```




---
:fire:
---