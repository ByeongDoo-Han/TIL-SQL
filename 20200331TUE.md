:v:
---
# parallel?
SQL을 처리할 때도 병렬처리를 통해 CPU 사용한도를 늘려서 빨리 일을 처리 할 수 있다.
현업에서는 못쓴다고 보면 된다.
randomForest는 각 트리를 독립적으로 이용하기 때문에 parallel을 적용하기에 좋은 모델이다.

# 분석 시 기타 고려사항
1. sampling 할때마다 다른 모델이 생성되고 평가점수가 달라지는 현상
    - cross validation(CV, 교차검증)을 통해서 평가점수를 평균내서 일반화하는 방식을 이용한다.
2. 변수 조합, 변형 고려
    - 분석프로젝트에서 변수연구는 매우 중요하다. :star:
    `ex) 지하주차장수용 * 강수량 -> 매우 유의하게 변한다.`

# 불순도?
하위 노드에 서로 다른 class로 구성되어 있는 정도를 나타낸 수치

[Blog - 지니불순도란?](https://smalldataguru.com/%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%ACdecision-tree%EC%9D%98-%EC%A7%80%EB%8B%88-%EB%B6%88%EC%88%9C%EB%8F%84gini-impurity%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C/)

- 지니 불순도 
대표적인 불순도 측정 지수
     

- 지니 불순도 측정방식
    y가 2개의 class로 구성되어 있는 경우
    `f(p) = p(1-p)`, p는 특정 class가 속할 확률을 의미한다.


---
# randomForest
    - dicision tree의 단일 의사결정 방식을 보완, 다수의 tree 구성
    - 여러개의 tree를 구성하여 종합 결론을 내는 방식
    - randomForest-regressor, randomForest-classifier 존재
    - 평균 또는 투표에 의해 최종 결론
    - 서로 다른 트리를 구성
```r
install.packages('randomForest')
library(randomForest)
```

## **1. sampling**
```r
v_rn <- sample(1:nrow(iris), size = nrow(iris) * 0.7)
tr_iris <- iris[v_rn, ]
te_iris <- iris[-v_rn, ]
```

## **2. training**
```r
m_rf <- randomForest(Species ~., data=tr_iris)
```



## **3. predict**
```r
v_rn2 <- sample(1:nrow(te_iris), size = 1)
v_newdata <- te_iris[v_rn2, ]

predict(m_rf, newdata = v_newdata)
v_newdata$Species
# v_newdata) == v_newdata$Species ? 확인
```

## **4. score**
```r
v_predict <- predict(m_rf, newdata = te_iris)
sum(te_iris$Species == v_predict) / nrow(te_iris) * 100
```


## **5. 매개변수 튜닝**
### 5.1 `ntree`
- **트리의 개수를 설정하는 변수**

#### elbow point 확인
> elbow point란? 
 예측률의 증가율이 큰폭으로 감소하는 지점 => 적절한 트리 수의 개수를 설정할 수 있는 근거가 된다.

```r
v_score <- c()
for (i in 1:100) {
    m_rf <- randomForest(Species ~., data = tr_iris, ntree = i)
    v_pr <- predict(m_rf, newdata = te_iris, type = 'class')
    vscore <- sum(te_iris$Species == v_pr) / nrow(te_iris) * 100
    v_score <- c(v_score, vscore)
}

m_dt$variable.importance
dev.new()

plot(1:100, v_score, type = 'o',
     xlab = 'ntree', ylab = 'score',
     main = 'RF에서의 ntree변화에 따른 예측률 변화')
```

### 5.2 `mtry`
- **각 노드에 설명변수 선택시 고려되는 후보의 개수**

>  클수록 많은 설명변수를 고려,
작을수록 전체 중 랜덤하게 선택된 일부 설명변수만 고려,
따라서 클수록 계속 비슷한 트리로 구성된다
작을수록 서로 다른 트리로 구성되고 각 트리는 매우 복잡해질 가능성 높음

```r
v_score <- c()
for (i in 1:4) {
    m_rf <- randomForest(Species ~., data = tr_iris, mtry = i)
    v_pr <- predict(m_rf, newdata = te_iris, type = 'class')
    vscore <- sum(te_iris$Species == v_pr) / nrow(te_iris) * 100
    v_score <- c(v_score, vscore)
}

dev.new()
plot(1:4, v_score, type = 'o',
     xlab = 'ntree', ylab = 'score',
     main = 'RF에서의 mtry변화에 따른 예측률 변화')
```





## 트리 기반 모델의 변수 선택 기능
트리기반 모델은 모델 내 변수의 중요도를 파악해 해당 변수 중 중요한 변수들을 선택하여 배치하는 기능을 가진다.
- 다른 모델에 비해 변수 선택에 대한 부담이 없음
- 최종모델로 사용하지 않더라도 변수 연구 시에 사용한다.



```
ex) (ooxxx)로 구성된 노드의 지니불순도 계산
p=o 가 선택될 확률이라 가정, p=2/5, 1-p=3/5
f(p) = 2/5 * 3/5 = 0.24
```

## 트리기반 모델 내부의 변수 중요도 확인
`randomForest`에서
`m_dt$importance`

`rpart`에서
`m_dt$variable.importance`


## randomForest - 회귀분석
```r
data('airquality')
airquality
m_rf_reg <- randomForest(Ozone ~. , 
                         data = airquality, 
                         mtry = 3,
                         na.action = na.omit) # na는 제거하겠다는 뜻
m_rf_reg
```

```
> m_rf_reg

Call:
 randomForest(formula = Ozone ~ ., data = airquality, mtry = 3,      na.action = na.omit) 
               Type of random forest: regression 

############## y가 연속형이기 때문에 regression으로 표현
############## 때문에 실제값과 오차제곱(잔차)의 평균을 비교해 평가한다.
############## 반면에 분류분석에서는 일치, 불일치로 판단한다.

                     Number of trees: 500
No. of variables tried at each split: 3

          Mean of squared residuals: 288.2783
                    % Var explained: 73.73
###########  ===> 회귀모델로 데이터의 73.73 퍼센트를 설명할 수 있다.
```



---
:fire:
---
